{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('dataset/iris.data')\n",
    "df = pd.read_csv(p)\n",
    "feature_cols = ['sepal_length', 'sepal_width','petal_length','petal_witdh']\n",
    "target_cols = ['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisDataset(data.Dataset):\n",
    "    def __init__(\n",
    "            self, path:str, feature_cols:list, \n",
    "            target_cols:list, clazz:list, \n",
    "            transforms_feature=None, transforms_target=None):\n",
    "        \n",
    "        self.path = Path(path)\n",
    "        self.dframe = pd.read_csv(self.path)\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_cols = target_cols\n",
    "        self.clazz = clazz\n",
    "        self.transforms_feature = transforms_feature\n",
    "        self.transforms_target = transforms_target\n",
    "        \n",
    "        self.__normalize_target()\n",
    "        self.class_to_idx = self.__class_to_label()\n",
    "        self.idx_to_class = self.__idx_to_class()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dframe)\n",
    "    \n",
    "    def __class_to_label(self):\n",
    "        mapz = [(val, idx) for idx, val in enumerate(self.clazz)]\n",
    "        return dict(mapz)\n",
    "    \n",
    "    def __idx_to_class(self):\n",
    "        mapz = [(idx, val) for idx, val in enumerate(self.clazz)]\n",
    "        return dict(mapz)\n",
    "    \n",
    "    def __normalize_target(self):\n",
    "        cat_type = CategoricalDtype(categories=self.clazz, ordered=True)\n",
    "        self.dframe[self.target_cols[0]] = self.dframe[self.target_cols[0]].astype(cat_type).cat.codes\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.dframe[self.feature_cols].iloc[idx].values\n",
    "        target = self.dframe[self.target_cols].iloc[idx].values\n",
    "        target = np.squeeze(target)\n",
    "        \n",
    "        if self.transforms_feature:\n",
    "            feature = self.transforms_feature(feature)\n",
    "        if self.transforms_target:\n",
    "            target = self.transforms_target(target)\n",
    "            \n",
    "        return feature, target\n",
    "\n",
    "\n",
    "def indice_splitter(dataset, valid_size, shuflle=True):\n",
    "    num_data = len(dataset)\n",
    "    indices = list(range(num_data))\n",
    "    split = int(np.floor(valid_size * num_data))\n",
    "    if shuflle:\n",
    "        np.random.seed(1)\n",
    "        np.random.shuffle(indices)\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    return train_idx, valid_idx\n",
    "\n",
    "class NumpyToFloatTensor(object):\n",
    "    def __call__(self, param):\n",
    "        return torch.from_numpy(param.astype(np.float32)).float()\n",
    "\n",
    "class NumpyToLongTensor(object):\n",
    "    def __call__(self, param):\n",
    "        return torch.from_numpy(param.astype(np.long)).long()\n",
    "\n",
    "        \n",
    "        \n",
    "path = 'dataset/iris.data'\n",
    "feature_cols = ['sepal_length', 'sepal_width','petal_length','petal_witdh']\n",
    "target_cols = ['species']\n",
    "clazz = [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n",
    "\n",
    "iris_dataset = IrisDataset(\n",
    "    path, feature_cols, \n",
    "    target_cols, clazz, \n",
    "    transforms_feature=NumpyToFloatTensor(), transforms_target=NumpyToLongTensor())\n",
    "\n",
    "train_idx, valid_idx = indice_splitter(iris_dataset, valid_size=0.2)\n",
    "\n",
    "train_loader = data.DataLoader(iris_dataset, batch_size=128, sampler=SubsetRandomSampler(train_idx), num_workers=0)\n",
    "valid_loader = data.DataLoader(iris_dataset, batch_size=128, sampler=SubsetRandomSampler(valid_idx), num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = nn.Linear(4,64)\n",
    "hidden_layer = nn.Linear(64,64)\n",
    "output_layer = nn.Linear(64,3)\n",
    "model = nn.Sequential(input_layer,nn.ReLU(inplace=True), hidden_layer,\n",
    "                      nn.ReLU(inplace=True), output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisNetwork(nn.Module):\n",
    "    def __init__(self, input, hidden, output):\n",
    "        super(IrisNetwork, self).__init__()\n",
    "        self.ln1 = nn.Linear(input, hidden)\n",
    "        self.relu1 = nn.ReLU(inplace=True) \n",
    "        self.ln2 = nn.Linear(hidden, hidden)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.ln3 = nn.Linear(hidden, output)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        x = self.ln1(input_data)\n",
    "        x = self.relu1(x)\n",
    "        x = self.ln2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.ln3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IrisNetwork(4, 32, 3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# titer = iter(train_loader)\n",
    "# feature,target = next(titer)\n",
    "# feature.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100 Batch: 0\t Loss: 1.0976 (1.0976)\n",
      "Epoch 1/100 Batch: 0\t Loss: 1.0924 (1.0924)\n",
      "Epoch 2/100 Batch: 0\t Loss: 1.0827 (1.0827)\n",
      "Epoch 3/100 Batch: 0\t Loss: 1.0704 (1.0704)\n",
      "Epoch 4/100 Batch: 0\t Loss: 1.0632 (1.0632)\n",
      "Epoch 5/100 Batch: 0\t Loss: 1.0576 (1.0576)\n",
      "Epoch 6/100 Batch: 0\t Loss: 1.0521 (1.0521)\n",
      "Epoch 7/100 Batch: 0\t Loss: 1.0462 (1.0462)\n",
      "Epoch 8/100 Batch: 0\t Loss: 1.0387 (1.0387)\n",
      "Epoch 9/100 Batch: 0\t Loss: 1.0289 (1.0289)\n",
      "Epoch 10/100 Batch: 0\t Loss: 1.0163 (1.0163)\n",
      "Epoch 11/100 Batch: 0\t Loss: 1.0008 (1.0008)\n",
      "Epoch 12/100 Batch: 0\t Loss: 0.9827 (0.9827)\n",
      "Epoch 13/100 Batch: 0\t Loss: 0.9624 (0.9624)\n",
      "Epoch 14/100 Batch: 0\t Loss: 0.9400 (0.9400)\n",
      "Epoch 15/100 Batch: 0\t Loss: 0.9154 (0.9154)\n",
      "Epoch 16/100 Batch: 0\t Loss: 0.8901 (0.8901)\n",
      "Epoch 17/100 Batch: 0\t Loss: 0.8610 (0.8610)\n",
      "Epoch 18/100 Batch: 0\t Loss: 0.8263 (0.8263)\n",
      "Epoch 19/100 Batch: 0\t Loss: 0.7877 (0.7877)\n",
      "Epoch 20/100 Batch: 0\t Loss: 0.7461 (0.7461)\n",
      "Epoch 21/100 Batch: 0\t Loss: 0.7022 (0.7022)\n",
      "Epoch 22/100 Batch: 0\t Loss: 0.6554 (0.6554)\n",
      "Epoch 23/100 Batch: 0\t Loss: 0.6068 (0.6068)\n",
      "Epoch 24/100 Batch: 0\t Loss: 0.5585 (0.5585)\n",
      "Epoch 25/100 Batch: 0\t Loss: 0.5112 (0.5112)\n",
      "Epoch 26/100 Batch: 0\t Loss: 0.4666 (0.4666)\n",
      "Epoch 27/100 Batch: 0\t Loss: 0.4257 (0.4257)\n",
      "Epoch 28/100 Batch: 0\t Loss: 0.3893 (0.3893)\n",
      "Epoch 29/100 Batch: 0\t Loss: 0.3586 (0.3586)\n",
      "Epoch 30/100 Batch: 0\t Loss: 0.3306 (0.3306)\n",
      "Epoch 31/100 Batch: 0\t Loss: 0.3033 (0.3033)\n",
      "Epoch 32/100 Batch: 0\t Loss: 0.2788 (0.2788)\n",
      "Epoch 33/100 Batch: 0\t Loss: 0.2574 (0.2574)\n",
      "Epoch 34/100 Batch: 0\t Loss: 0.2355 (0.2355)\n",
      "Epoch 35/100 Batch: 0\t Loss: 0.2117 (0.2117)\n",
      "Epoch 36/100 Batch: 0\t Loss: 0.1913 (0.1913)\n",
      "Epoch 37/100 Batch: 0\t Loss: 0.1749 (0.1749)\n",
      "Epoch 38/100 Batch: 0\t Loss: 0.1582 (0.1582)\n",
      "Epoch 39/100 Batch: 0\t Loss: 0.1417 (0.1417)\n",
      "Epoch 40/100 Batch: 0\t Loss: 0.1299 (0.1299)\n",
      "Epoch 41/100 Batch: 0\t Loss: 0.1190 (0.1190)\n",
      "Epoch 42/100 Batch: 0\t Loss: 0.1067 (0.1067)\n",
      "Epoch 43/100 Batch: 0\t Loss: 0.0986 (0.0986)\n",
      "Epoch 44/100 Batch: 0\t Loss: 0.0929 (0.0929)\n",
      "Epoch 45/100 Batch: 0\t Loss: 0.0851 (0.0851)\n",
      "Epoch 46/100 Batch: 0\t Loss: 0.0809 (0.0809)\n",
      "Epoch 47/100 Batch: 0\t Loss: 0.0792 (0.0792)\n",
      "Epoch 48/100 Batch: 0\t Loss: 0.0747 (0.0747)\n",
      "Epoch 49/100 Batch: 0\t Loss: 0.0722 (0.0722)\n",
      "Epoch 50/100 Batch: 0\t Loss: 0.0731 (0.0731)\n",
      "Epoch 51/100 Batch: 0\t Loss: 0.0706 (0.0706)\n",
      "Epoch 52/100 Batch: 0\t Loss: 0.0703 (0.0703)\n",
      "Epoch 53/100 Batch: 0\t Loss: 0.0726 (0.0726)\n",
      "Epoch 54/100 Batch: 0\t Loss: 0.0707 (0.0707)\n",
      "Epoch 55/100 Batch: 0\t Loss: 0.0722 (0.0722)\n",
      "Epoch 56/100 Batch: 0\t Loss: 0.0744 (0.0744)\n",
      "Epoch 57/100 Batch: 0\t Loss: 0.0731 (0.0731)\n",
      "Epoch 58/100 Batch: 0\t Loss: 0.0778 (0.0778)\n",
      "Epoch 59/100 Batch: 0\t Loss: 0.0764 (0.0764)\n",
      "Epoch 60/100 Batch: 0\t Loss: 0.0805 (0.0805)\n",
      "Epoch 61/100 Batch: 0\t Loss: 0.0804 (0.0804)\n",
      "Epoch 62/100 Batch: 0\t Loss: 0.0844 (0.0844)\n",
      "Epoch 63/100 Batch: 0\t Loss: 0.0853 (0.0853)\n",
      "Epoch 64/100 Batch: 0\t Loss: 0.0893 (0.0893)\n",
      "Epoch 65/100 Batch: 0\t Loss: 0.0884 (0.0884)\n",
      "Epoch 66/100 Batch: 0\t Loss: 0.0955 (0.0955)\n",
      "Epoch 67/100 Batch: 0\t Loss: 0.0929 (0.0929)\n",
      "Epoch 68/100 Batch: 0\t Loss: 0.1027 (0.1027)\n",
      "Epoch 69/100 Batch: 0\t Loss: 0.1051 (0.1051)\n",
      "Epoch 70/100 Batch: 0\t Loss: 0.1024 (0.1024)\n",
      "Epoch 71/100 Batch: 0\t Loss: 0.1214 (0.1214)\n",
      "Epoch 72/100 Batch: 0\t Loss: 0.1310 (0.1310)\n",
      "Epoch 73/100 Batch: 0\t Loss: 0.1128 (0.1128)\n",
      "Epoch 74/100 Batch: 0\t Loss: 0.1786 (0.1786)\n",
      "Epoch 75/100 Batch: 0\t Loss: 0.2041 (0.2041)\n",
      "Epoch 76/100 Batch: 0\t Loss: 0.1933 (0.1933)\n",
      "Epoch 77/100 Batch: 0\t Loss: 0.2761 (0.2761)\n",
      "Epoch 78/100 Batch: 0\t Loss: 0.1600 (0.1600)\n",
      "Epoch 79/100 Batch: 0\t Loss: 0.1654 (0.1654)\n",
      "Epoch 80/100 Batch: 0\t Loss: 0.3142 (0.3142)\n",
      "Epoch 81/100 Batch: 0\t Loss: 0.2632 (0.2632)\n",
      "Epoch 82/100 Batch: 0\t Loss: 0.2695 (0.2695)\n",
      "Epoch 83/100 Batch: 0\t Loss: 0.3590 (0.3590)\n",
      "Epoch 84/100 Batch: 0\t Loss: 0.2261 (0.2261)\n",
      "Epoch 85/100 Batch: 0\t Loss: 0.2036 (0.2036)\n",
      "Epoch 86/100 Batch: 0\t Loss: 0.5449 (0.5449)\n",
      "Epoch 87/100 Batch: 0\t Loss: 0.3351 (0.3351)\n",
      "Epoch 88/100 Batch: 0\t Loss: 0.3909 (0.3909)\n",
      "Epoch 89/100 Batch: 0\t Loss: 0.4219 (0.4219)\n",
      "Epoch 90/100 Batch: 0\t Loss: 0.2864 (0.2864)\n",
      "Epoch 91/100 Batch: 0\t Loss: 0.2277 (0.2277)\n",
      "Epoch 92/100 Batch: 0\t Loss: 0.8140 (0.8140)\n",
      "Epoch 93/100 Batch: 0\t Loss: 0.4986 (0.4986)\n",
      "Epoch 94/100 Batch: 0\t Loss: 0.6403 (0.6403)\n",
      "Epoch 95/100 Batch: 0\t Loss: 0.2652 (0.2652)\n",
      "Epoch 96/100 Batch: 0\t Loss: 0.2020 (0.2020)\n",
      "Epoch 97/100 Batch: 0\t Loss: 0.2326 (0.2326)\n",
      "Epoch 98/100 Batch: 0\t Loss: 0.5558 (0.5558)\n",
      "Epoch 99/100 Batch: 0\t Loss: 0.7755 (0.7755)\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "for epoch in range(num_epoch):\n",
    "    losses = AverageMeter()\n",
    "    for idx, (feature, target) in enumerate(train_loader):\n",
    "        pred = model(feature)\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        bsize = feature.size(0)\n",
    "        losses.update(loss.item(), bsize)\n",
    "        print(f'Epoch {epoch}/{num_epoch} Batch: {idx*64}\\t Loss: {loss.item():.4f} ({losses.avg:.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
